\label{abstractSection}

\begin{abstract}


In the field of speech recognition, an algorithm must learn to tell the difference between "a nice rock" and "a gneiss rock". These identical-sounding phrases are called oronyms. Word frequency dictionaries are often used by speech recognition systems to help resolve phonetic sequences with more than one possible orthographic phrase interpretation, by looking up which oronym of the root phonetic sequence contains the most common words. However, this approach is highly dependent upon the manner in which the frequency values in a word frequency dictionary are obtained. 

Our paper demonstrates a technique used to validate word frequency dictionary values. We use oronym trees to compare phrase frequency values from dictionaries, to the frequency with which our human test subjects heard different variations of the root phrase. We chose to use frequency values from the UNISYN dictionary, which uses tallies each word occurance in a propriatary text corpus (***Glossary term***?).

Given any valid English phrase, herein referred to as the root phrase, our system will first generate all possible correct phonetic sequences for a General American accent. Then, it parses through these phonetic transcriptions depth-first, looking for valid orthographic words for each subsequent phonetic subsequence, generating full and partial phrases from these words.  In the event that the entire phonetic sequence branch can be parsed into a valid orthographic phrase, we save this orthographic phrase as an oronym of the root phrase.

We also developed a visual representation of the oronym trees, to allow for visualizing phonetic dead-ends.  In the event that a branch's phonetic ``tail" is not orthographically interpretable, we visually ``dead-end" the branch by drawing a red sphere. A particularly strong orthographic partial phrase before a phonetic dead-end can mislead a listener, causing them to lose track of the words in the rest of the phrase.   In the event that the entire phonetic sequence can be parsed into a valid orthographic phrase, we indicate this successfully-found oronym with a green sphere.  

Using the oronyms generated from our oronym tree, we then conducted a user study. Our multi-phase user study, incorporated over \numResponsesPhaseTwoUserStudy data points from \uniqueUsersPhaseTwoUserStudy test subjects.  In it, we tested the validity of our oronym generation by having participants record themselves reading an oronym phrase. Then, a second set of subjects transcribed the recordings.  

In the first phase, we generated oronym strings for the phrase \emph{``a nice cold hour''},  and had over \uniqueUsersPhaseOneUserStudy people make \numResponsesPhaseOneUserStudy recordings of the most common oronyms for that phrase. We then compared their pronunciations to the pronunciations we were expecting, and found that in \numCorrectPronunciationPhaseOneUserStudy cases, the recorded phrase's phonemics matched our expectation.  This indicates that, while not exhaustive, our pronunciation dictionary is a good match for actual American-English pronunciations. In the second phase, we selected \recordingsPhaseTwoUserStudy of the phase one recordings, and had \numTranscriptionsPerRecordingPhaseTwoUserStudy different people transcribe each one.  

If the frequency dictionary values for our test phrases accurately reflect the real-world expectations of actual listeners, we would expect that the most commonly transcribed phrases in our user study would roughly correspond with our metric for the most likely oronym interpretation of the root phrase. 

The best possible use case to show this is the case of \emph{``a nice cold hour''}, whose commonly-misheard **********(find citation)*************** oronym is \emph{``an ice cold hour''}. The words  ``a'' and ``an'' are identical in function, but ``an'' is only used in the case that the following word starts with a vowel sound. As there are more consonant sounds than vowel sounds, ``an'' is used far less often than ``a'' is. The UNISYN dictionary has a frequency value of 7,536,297 for ``a'', and of 794,169 for ``an'', for a proportion of _______ to ______, or ____decimal_____ 

In this case, we'd expect that the phrase  \emph{``\emph{a} nice cold hour''} would be transcribed nearly ten times as often as  \emph{``\emph{an} ice cold hour''}.

In the event that this was not the case, we can conclude that tally-per-occurance frequency dictionary values does not apply well to an average audience's auditory expectations. 

During the course of our study, we found that the presence of excessively-common words (such as ``the'', ``is'', and ``a'' ) threw off our frequency metric when we used per-occurance frequency value. These super-common words have such high per-occurance tallies that it overpowered the effect that any regular word had on a frequency metric.  However, when we used per-document frequency values, we found that this effected was mitigated.  

The ____ frequency dictionary tallies the number of documents that a word is found in, instead of tallying the total number occurances of that over all documents.  In this dictionary, ``a'' h as a frequency value of _________, and ``an'' has a frequency value of of ______, for a proportion of _______ to ______, or ____decimal_____.

In our user study, we found that _____ people transcribed \phrase1, and _____ people transcribed \phrase2, for a proportion of _______ to ______, or ____decimal_____. We did a statistical test with a alpha of .01, and got a value that was so low we can't find a calculator that has enough decimal places to show it without rounding it to zero. In short, our per-occurance frequency metric predictions don't even remotely match the projected data. 

When compared to per-document frequency metric preditions, we got a p-value of ____, which is slightly better, but not great. 




We found that using per-occurance frequency values when computing our overall-phrase-frequency metric caused the thrown off by excessively common words, such as ``the'', ``is'', and ``a''. These super-common words have such high per-occurance tallies that it overpowered the effect that any regular word had on a frequency metric. However, when we tally on a per-document basis, instead of a by-occurance basis, we found that this effect was mitigated. 


I need help with statistics here.
To facilitate comparison, we created two rankings for the actual result phrases: one list ranked by expected frequency, and one ranked by number of actual transcriptions by our test subjects. In our phase two results, we found that out of the \numTotalTranscriptionsPhaseTwo transcriptions acquired for \numUniqueTranscriptionsPhaseTwo unique phrases, only \numUniqueTranscriptionsWithinEpsilonPhaseTwo had less than a difference of \phaseTwoToleratedEpsilon ranks between the actual and expected occurances. The top 10 unique results, accounting for 88.00\% of total transcriptions, were on average 25 ranks more common than the frequency metric ranking predicted they'd be, with over half of them more than 39 ranks higher (out of \numUniqueTranscriptionsPhaseTwo total ranks).   From this, we can conclude that the frequency dictionary that we used is flawed.


If these frequency dictionary values were correct for the phrase words, we would expect that the most commonly transcribed phrases in our user study would roughly correspond with our metric for the most likely oronym interpretation of the root phrase. In the event that this was not the case, we could conclude that the frequency dictionary values were in error for that phrase. To facilitate comparison, we created two rankings for the actual result phrases: one list ranked by expected frequency, and one ranked by number of actual transcriptions by our test subjects. In our phase two results, we found that out of the \numTotalTranscriptionsPhaseTwo transcriptions acquired for \numUniqueTranscriptionsPhaseTwo unique phrases, only \numUniqueTranscriptionsWithinEpsilonPhaseTwo had less than a difference of \phaseTwoToleratedEpsilon ranks between the actual and expected occurances. The top 10 unique results, accounting for 88.00\% of total transcriptions, were on average 25 ranks more common than the frequency metric ranking predicted they'd be, with over half of them more than 39 ranks higher (out of \numUniqueTranscriptionsPhaseTwo total ranks).   From this, we can conclude that the frequency dictionary that we used is flawed.

\end{abstract}
â€‹